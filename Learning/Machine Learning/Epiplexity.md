
### Basic
- Proposed from [From Entropy to Epiplexity Rethinking Information for Computationally Bounded Intelligence (2601.03220v1)](../../Literatures/Papers/From%20Entropy%20to%20Epiplexity%20Rethinking%20Information%20for%20Computationally%20Bounded%20Intelligence%20(2601.03220v1).md)
- Meaning: Learnable structured information


### Comparison with [Entropy](Entropy.md)
- Entropy measures uncertainty in information, a classical theory in information theory

### Three seemingly paradoxes in Information Theory
1. Information cannot be increased by deterministic processes.
	> **Example**: Inside $y=f(x)$, the information contained in cannot exceed the sum of that in $f$ and $x$
	> **Example**: The training of AlphaGo with [GAN](GAN.md) involves only computation without data, so where does the "intelligence" comes from?
2. Information is independent of factorization order.


### 參考
- [Youtube 論文解讀](https://www.youtube.com/watch?v=5xP7fZMaYmI)

Entropy