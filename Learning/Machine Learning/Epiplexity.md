
### Basic
- Proposed from [From Entropy to Epiplexity Rethinking Information for Computationally Bounded Intelligence (2601.03220v1)](../../Literatures/Papers/From%20Entropy%20to%20Epiplexity%20Rethinking%20Information%20for%20Computationally%20Bounded%20Intelligence%20(2601.03220v1).md)
- Meaning: Learnable structured information


### Comparison with [Entropy](Entropy.md)
- Entropy measures uncertainty in information, a classical theory in information theory

### Three seemingly paradoxes in Information Theory
1. Information cannot be increased by deterministic processes.
	> **Example**: Inside $y=f(x)$, the information contained in cannot exceed the sum of that in $f$ and $x$
	> **Example**: The training of AlphaGo with [GAN](GAN.md) involves only computation without data, so where does the "intelligence" comes from?
2. Information is independent of factorization order.
	> **Example**: `I will go home tomorrow` is easy to learn for models, while `tomorrow go home will I` is not
3. Likelihood modeling is merely distribution matching
	> **Explanation**: Classical theory claims that models are just matching the underlying distribution of the data, so it should not be able to learn patterns that does not exhibit in the data, yet we observe 


### 參考
- [Youtube 論文解讀](https://www.youtube.com/watch?v=5xP7fZMaYmI)

Entropy